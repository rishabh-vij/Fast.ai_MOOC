{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled3.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rishabh-vij/Fast.ai_MOOC/blob/master/Week%206/Gradient_descent_pytorch_self.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "GK8ExHLwffaD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#!pip install opencv-python\n",
        "#!apt update && apt install -y libsm6 libxext6\n",
        "!pip3 install http://download.pytorch.org/whl/cu80/torch-0.3.0.post4-cp36-cp36m-linux_x86_64.whl \n",
        "! pip3 install fastai\n",
        "#!pip3 install torchvision\n",
        "#! pip install torchtext"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M09yUJjjhAua",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient descent and Linear regresson from scratch with pytorch\n",
        "**The goal of linear regression is to fit a line to a set of points.**"
      ]
    },
    {
      "metadata": {
        "id": "Nll42fqigF0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "from fastai.learner import *\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SYs8abp8hJ76",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "670075ad-335e-414a-f58c-748fa10dce0a"
      },
      "cell_type": "code",
      "source": [
        "# Here we generate some fake data\n",
        "def lin(a,b,x): return a*x+b\n",
        "\n",
        "def gen_fake_data(n, a, b):\n",
        "    x = s = np.random.uniform(0,1,n) \n",
        "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
        "    return x, y\n",
        "\n",
        "x, y = gen_fake_data(50, 3., 8.)\n",
        "plt.scatter(x,y, s=8); plt.xlabel(\"x\"); plt.ylabel(\"y\");"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nmSY3BzcAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f0e9d29ba58>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "TLgJjKGMiKGf",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        " **We want to find parameters (weights) $a$ and $b$ such that we minimize the error between the points and the line $a\\cdot x + b$. Note that here $a$ and $b$ are unknown. For a regression problem the most common error function or loss function is the mean squared error.**\n",
        " \n",
        " \n",
        "**Suppose we believe $a = 10$ and $b = 5$ then we can compute y_hat which is our prediction and then compute our error.**\n"
      ]
    },
    {
      "metadata": {
        "id": "8B7ju--jhb60",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c19fa6fd-63ec-4408-cbfb-b2149b95ad05"
      },
      "cell_type": "code",
      "source": [
        "def mse(y_hat, y): return ((y_hat - y) ** 2).mean()\n",
        "\n",
        "y_hat = lin(10,5,x)\n",
        "mse(y_hat, y)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6313905096639107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "fEP3GhMdh2Wn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c2260abf-89b2-44c5-9c01-e0a0973ebf1f"
      },
      "cell_type": "code",
      "source": [
        "def mse_loss(a, b, x, y): return mse(lin(a,b,x), y) # This step is the same as above block\n",
        "mse_loss(10,5,x,y)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3.6313905096639107"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "pYk7nwKktAsq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Now that we've our linear regression function(lin) and loss function(mse_loss) defined, we need an optimizer that finds best values for $a$ and $b$  **\n",
        "\n",
        "##Gradient Descent optimizer\n",
        "\n",
        "**For a fixed dataset $x$ and $y$ mse_loss(a,b) is a function of $a$ and $b$. We would like to find the values of $a$ and $b$ that minimize that function.**\n",
        "\n",
        "**Gradient descent is an algorithm that minimizes functions. Given a function defined by a set of parameters, gradient descent starts with an initial set of parameter values and iteratively moves toward a set of parameter values that minimize the function. This iterative minimization is achieved by taking steps in the negative direction of the function gradient.**"
      ]
    },
    {
      "metadata": {
        "id": "syFjLAMTjHX-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d664cbfe-a8f2-48f9-e030-215834f1dbdb"
      },
      "cell_type": "code",
      "source": [
        "#Let's start by generating a little bigger data\n",
        "x,y=gen_fake_data(10000,3.0,8.0)\n",
        "x.shape,y.shape,type(x),type(y)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((10000,), (10000,), numpy.ndarray, numpy.ndarray)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "metadata": {
        "id": "fm3r1iWtuPil",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,y=V(x),V(y) #converting np.ndarray into pytorch variables. P.S: This step is not needed after pytorch v0.4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tCQWjgPCuZA4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "453369fb-c3af-4cd5-ae23-a43fa79cb12e"
      },
      "cell_type": "code",
      "source": [
        "a=V(np.random.randn(1),requires_grad=True)\n",
        "b=V(np.random.randn(1),requires_grad=True) #starting with random a and b weights of type pytorch variable\n",
        "a,b"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Variable containing:\n",
              " -2.4541\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:\n",
              " -1.0557\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "metadata": {
        "id": "4UNhKbFJvkid",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "027e4b94-1949-4f07-9bf0-0e04b2651ad3"
      },
      "cell_type": "code",
      "source": [
        "lr=1e-3\n",
        "for t in range(10000):    # Forward pass: compute predicted y using operations on Variables\n",
        "  loss=mse_loss(a,b,x,y)\n",
        "  if (t%1000==0): print(loss.data[0]) \n",
        "  \n",
        "  # Computes the gradient of loss with respect to all Variables with requires_grad=True.\n",
        "  # After this call a.grad and b.grad will be Variables holding the gradient\n",
        "  # of the loss with respect to a and b respectively\n",
        "  loss.backward()\n",
        "  \n",
        "  \n",
        "  # Update a and b using gradient descent; a.data and b.data are Tensors,\n",
        "  # a.grad and b.grad are Variables and a.grad.data and b.grad.data are Tensors\n",
        "  a.data= a.data-lr*a.grad.data\n",
        "  b.data= b.data-lr*b.grad.data\n",
        "  \n",
        "  # Zero the gradients\n",
        "  a.grad.data.zero_()\n",
        "  b.grad.data.zero_()\n"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "141.51861572265625\n",
            "0.9813310503959656\n",
            "0.10777968913316727\n",
            "0.09989085048437119\n",
            "0.09793990105390549\n",
            "0.09646869450807571\n",
            "0.09533698856830597\n",
            "0.0944683626294136\n",
            "0.0938015878200531\n",
            "0.09328839927911758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gNRQ2xqZwq3g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "f3b3f9d8-0a15-4ad6-f1f8-87e60de6a665"
      },
      "cell_type": "code",
      "source": [
        "a,b"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Variable containing:\n",
              "  2.8795\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:\n",
              "  8.0599\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "metadata": {
        "id": "tWJORI882_QB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gradient Descent for Classification with pytorch\n",
        "**Everything stays same as above, only we change the loss function for our classification problem**\n"
      ]
    },
    {
      "metadata": {
        "id": "vK-q6wSK1JRq",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gen_fake_data2(n, a, b):\n",
        "    x = s = np.random.uniform(0,1,n) \n",
        "    y = lin(a,b,x) + 0.1 * np.random.normal(0,3,n)\n",
        "    return x, np.where(y>10, 1, 0).astype(np.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lEXvJaen1KRj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "x,y = gen_fake_data2(10000, 3., 8.)\n",
        "x,y = V(x),V(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sR6f3zkC5l4N",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8a8e88a-8a6a-4c99-8f27-338d09f78e33"
      },
      "cell_type": "code",
      "source": [
        "x.shape,y.shape"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([10000]), torch.Size([10000]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "metadata": {
        "id": "nviZ_QPV3Lx4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def nll(y_hat, y):\n",
        "    y_hat = torch.clamp(y_hat, 1e-5, 1-1e-5)\n",
        "    return (y*y_hat.log() + (1-y)*(1-y_hat).log()).mean()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RM2rdBG64z24",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "a = V(np.random.randn(1), requires_grad=True)\n",
        "b = V(np.random.randn(1), requires_grad=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c9VswGyk5GFG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "cd125095-56bd-4c0c-c29e-0fb622892e3b"
      },
      "cell_type": "code",
      "source": [
        "a,b"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(Variable containing:\n",
              " -1.5738\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)], Variable containing:\n",
              "  0.9355\n",
              " [torch.cuda.FloatTensor of size 1 (GPU 0)])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 101
        }
      ]
    },
    {
      "metadata": {
        "id": "pBECO1qx5HO3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "ee9da80f-a8ce-42eb-e3dc-6095798760af"
      },
      "cell_type": "code",
      "source": [
        "lr = 1e-2\n",
        "for t in range(3000):\n",
        "    p = (-lin(a,b,x)).exp()\n",
        "    y_hat = 1/(1+p)# to convert predictions between 0 and 1(classification problem)\n",
        "    loss = nll(y_hat,y)\n",
        "    if t % 1000 == 0:\n",
        "        print(loss.data[0],     np.mean(to_np(y)==(to_np(y_hat)>0.5)))\n",
        "    \n",
        "    loss.backward()\n",
        "    a.data =a.data- lr * a.grad.data\n",
        "    b.data =b.data- lr * b.grad.data\n",
        "    a.grad.data.zero_()\n",
        "    b.grad.data.zero_()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.9157661199569702 0.0998\n",
            "-3.937028169631958 0.3301\n",
            "-7.708682060241699 0.3301\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tqkgbuDe7t3r",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**0.3301, Not bad**"
      ]
    },
    {
      "metadata": {
        "id": "8QDSLkeC72Rc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##End.\n",
        "\n",
        "\n"
      ]
    }
  ]
}